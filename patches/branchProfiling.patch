# HG changeset patch
# Parent 8b4d14afc4f6b726274897d47f0038e6959a90aa
# User Wei Wu <lazyparser@gmail.com>
Bug 877878 - Instrument Baseline generated code to collect branch profiles. r=nbp

diff --git a/js/src/jit/BaselineCompiler.cpp b/js/src/jit/BaselineCompiler.cpp
--- a/js/src/jit/BaselineCompiler.cpp
+++ b/js/src/jit/BaselineCompiler.cpp
@@ -152,17 +152,18 @@ BaselineCompiler::compile()
 
     prologueOffset_.fixup(&masm);
     spsPushToggleOffset_.fixup(&masm);
 
     BaselineScript *baselineScript = BaselineScript::New(cx, prologueOffset_.offset(),
                                                          spsPushToggleOffset_.offset(),
                                                          icEntries_.length(),
                                                          pcMappingIndexEntries.length(),
-                                                         pcEntries.length());
+                                                         pcEntries.length(),
+                                                         blockCounterEntries_.length());
     if (!baselineScript)
         return Method_Error;
 
     baselineScript->setMethod(code);
 
     script->setBaselineScript(baselineScript);
 
     IonSpew(IonSpew_BaselineScripts, "Created BaselineScript %p (raw %p) for %s:%d",
@@ -192,27 +193,49 @@ BaselineCompiler::compile()
         label.fixup(&masm);
         size_t icEntry = icLoadLabels_[i].icEntry;
         ICEntry *entryAddr = &(baselineScript->icEntry(icEntry));
         Assembler::patchDataWithValueCheck(CodeLocationLabel(code, label),
                                            ImmPtr(entryAddr),
                                            ImmPtr((void*)-1));
     }
 
+    IonSpew(IonSpew_BranchProfiles, "%d counters emitted for script %s:%d (%p)",
+            blockCounterEntries_.length(), script->filename(),
+            script->lineno, script->baselineScript());
+
+    if (blockCounterEntries_.length())
+        baselineScript->copyBlockCounterEntries(&blockCounterEntries_[0]);
+
+    // Patch block counters
+    for (size_t i = 0; i < blockCounterLabels_.length(); i++) {
+        CodeOffsetLabel label = blockCounterLabels_[i].label;
+        label.fixup(&masm);
+        size_t bcEntry = blockCounterLabels_[i].bcEntry;
+        BlockCounterEntry *bcEntryAddr = &baselineScript->blockCounterEntry(bcEntry);
+        Assembler::patchDataWithValueCheck(CodeLocationLabel(code, label),
+                                           ImmPtr(bcEntryAddr),
+                                           ImmPtr((void*)-1));
+        bcEntryAddr->toggleOffset.fixup(&masm);
+    }
+
     if (modifiesArguments_)
         baselineScript->setModifiesArguments();
 
     // All barriers are emitted off-by-default, toggle them on if needed.
     if (cx->zone()->needsBarrier())
         baselineScript->toggleBarriers(true);
 
     // All SPS instrumentation is emitted toggled off.  Toggle them on if needed.
     if (cx->runtime()->spsProfiler.enabled())
         baselineScript->toggleSPS(true);
 
+    if (js_IonOptions.baselineBranchProfiling)
+        baselineScript->enableBranchProfiling();
+
     return Method_Compiled;
 }
 
 bool
 BaselineCompiler::emitPrologue()
 {
     masm.push(BaselineFrameReg);
     masm.mov(BaselineStackReg, BaselineFrameReg);
@@ -369,16 +392,46 @@ BaselineCompiler::emitIC(ICStub *stub, b
     EmitCallIC(&patchOffset, masm);
     entry->setReturnOffset(masm.currentOffset());
     if (!addICLoadLabel(patchOffset))
         return false;
 
     return true;
 }
 
+bool
+BaselineCompiler::emitBlockCounter(jsbytecode *pc)
+{
+    if (!ionCompileable_ && !ionOSRCompileable_)
+        return true;
+
+    IonSpew(IonSpew_BranchProfiles, "Emit a counter for op @ %d: %s",
+            int(pc - script->code), js_CodeName[JSOp(*pc)]);
+
+    BlockCounterEntry *entry = allocateBlockCounterEntry(pc - script->code);
+    if(!entry)
+        return false;
+
+    Label skipCount;
+    CodeOffsetLabel toggleOffset = masm.toggledJump(&skipCount);
+    entry->toggleOffset = toggleOffset;
+
+    Register addressReg = R1.scratchReg();
+    CodeOffsetLabel counterOffset = masm.movWithPatch(ImmWord(-1), addressReg);
+    Address counterAddr(addressReg, BlockCounterEntry::offsetOfCounter());
+    masm.add32(Imm32(1), counterAddr);
+
+    masm.bind(&skipCount);
+
+    if(!addBlockCounterLabel(counterOffset))
+        return false;
+
+    return true;
+}
+
 typedef bool (*CheckOverRecursedWithExtraFn)(JSContext *, uint32_t);
 static const VMFunction CheckOverRecursedWithExtraInfo =
     FunctionInfo<CheckOverRecursedWithExtraFn>(CheckOverRecursedWithExtra);
 
 bool
 BaselineCompiler::emitStackCheck()
 {
     Label skipCall;
@@ -674,16 +727,22 @@ BaselineCompiler::emitBody()
             emittedOps = 0;
         if (!addPCMappingEntry(addIndexEntry))
             return Method_Error;
 
         // Emit traps for breakpoints and step mode.
         if (debugMode_ && !emitDebugTrap())
             return Method_Error;
 
+        if (js_IonOptions.baselineBranchProfiling) {
+            // Instrument all jump targets and the first opcode.
+            if ((info->jumpTarget || pc == script->code) && !emitBlockCounter(pc))
+                return Method_Error;
+        }
+
         switch (op) {
           default:
             IonSpew(IonSpew_BaselineAbort, "Unhandled op: %s", js_CodeName[op]);
             return Method_CantCompile;
 
 #define EMIT_OP(OP)                            \
           case OP:                             \
             if (!this->emit_##OP())            \
diff --git a/js/src/jit/BaselineCompiler.h b/js/src/jit/BaselineCompiler.h
--- a/js/src/jit/BaselineCompiler.h
+++ b/js/src/jit/BaselineCompiler.h
@@ -209,16 +209,18 @@ class BaselineCompiler : public Baseline
     bool emitIC(ICStub *stub, bool isForOp);
     bool emitOpIC(ICStub *stub) {
         return emitIC(stub, true);
     }
     bool emitNonOpIC(ICStub *stub) {
         return emitIC(stub, false);
     }
 
+    bool emitBlockCounter(jsbytecode *pc);
+
     bool emitStackCheck();
     bool emitInterruptCheck();
     bool emitUseCountIncrement();
     bool emitArgumentTypeChecks();
     bool emitDebugPrologue();
     bool emitDebugTrap();
     bool emitSPSPush();
     void emitSPSPop();
diff --git a/js/src/jit/BaselineJIT.cpp b/js/src/jit/BaselineJIT.cpp
--- a/js/src/jit/BaselineJIT.cpp
+++ b/js/src/jit/BaselineJIT.cpp
@@ -41,17 +41,18 @@ PCMappingSlotInfo::ToSlotLocation(const 
 BaselineScript::BaselineScript(uint32_t prologueOffset, uint32_t spsPushToggleOffset)
   : method_(NULL),
     fallbackStubSpace_(),
     prologueOffset_(prologueOffset),
 #ifdef DEBUG
     spsOn_(false),
 #endif
     spsPushToggleOffset_(spsPushToggleOffset),
-    flags_(0)
+    flags_(0),
+    branchProfilingEnabled_(false)
 { }
 
 static const size_t BASELINE_LIFO_ALLOC_PRIMARY_CHUNK_SIZE = 4096;
 static const unsigned BASELINE_MAX_ARGS_LENGTH = 20000;
 
 static bool
 CheckFrame(StackFrame *fp)
 {
@@ -353,31 +354,35 @@ jit::CanEnterBaselineMethod(JSContext *c
 };
 
 // Be safe, align IC entry list to 8 in all cases.
 static const unsigned DataAlignment = sizeof(uintptr_t);
 
 BaselineScript *
 BaselineScript::New(JSContext *cx, uint32_t prologueOffset,
                     uint32_t spsPushToggleOffset, size_t icEntries,
-                    size_t pcMappingIndexEntries, size_t pcMappingSize)
+                    size_t pcMappingIndexEntries, size_t pcMappingSize,
+                    size_t blockCounterEntries)
 {
     size_t paddedBaselineScriptSize = AlignBytes(sizeof(BaselineScript), DataAlignment);
 
     size_t icEntriesSize = icEntries * sizeof(ICEntry);
     size_t pcMappingIndexEntriesSize = pcMappingIndexEntries * sizeof(PCMappingIndexEntry);
+    size_t blockCounterEntriesSize = blockCounterEntries * sizeof(BlockCounterEntry);
 
     size_t paddedICEntriesSize = AlignBytes(icEntriesSize, DataAlignment);
     size_t paddedPCMappingIndexEntriesSize = AlignBytes(pcMappingIndexEntriesSize, DataAlignment);
     size_t paddedPCMappingSize = AlignBytes(pcMappingSize, DataAlignment);
+    size_t paddedBlockCounterSize = AlignBytes(blockCounterEntriesSize, DataAlignment);
 
     size_t allocBytes = paddedBaselineScriptSize +
         paddedICEntriesSize +
         paddedPCMappingIndexEntriesSize +
-        paddedPCMappingSize;
+        paddedPCMappingSize +
+        paddedBlockCounterSize;
 
     uint8_t *buffer = (uint8_t *)cx->malloc_(allocBytes);
     if (!buffer)
         return NULL;
 
     BaselineScript *script = reinterpret_cast<BaselineScript *>(buffer);
     new (script) BaselineScript(prologueOffset, spsPushToggleOffset);
 
@@ -390,16 +395,20 @@ BaselineScript::New(JSContext *cx, uint3
     script->pcMappingIndexOffset_ = offsetCursor;
     script->pcMappingIndexEntries_ = pcMappingIndexEntries;
     offsetCursor += paddedPCMappingIndexEntriesSize;
 
     script->pcMappingOffset_ = offsetCursor;
     script->pcMappingSize_ = pcMappingSize;
     offsetCursor += paddedPCMappingSize;
 
+    script->blockCounterOffset_ = offsetCursor;
+    script->blockCounterEntries_ = blockCounterEntries;
+    offsetCursor += paddedBlockCounterSize;
+
     return script;
 }
 
 void
 BaselineScript::trace(JSTracer *trc)
 {
     MarkIonCode(trc, &method_, "baseline-method");
 
@@ -425,16 +434,32 @@ BaselineScript::writeBarrierPre(Zone *zo
 
 void
 BaselineScript::Trace(JSTracer *trc, BaselineScript *script)
 {
     script->trace(trc);
 }
 
 void
+BaselineScript::DumpBlockCounters(BaselineScript *script)
+{
+#ifdef DEBUG
+    BlockCounterEntry *entries = script->blockCounterEntryList();
+    size_t length = script->numBlockCounters();
+    IonSpew(IonSpew_BranchProfiles,
+            "DumpBlockCounters Statistics %zu counters for baselineScript %p",
+            length, script);
+    for (size_t i = 0;i < length; i++) {
+        IonSpew(IonSpew_BranchProfiles, "DumpBlockCounters offset %zu value %u",
+                entries[i].pcOffset, entries[i].counter);
+    }
+#endif
+}
+
+void
 BaselineScript::Destroy(FreeOp *fop, BaselineScript *script)
 {
     fop->delete_(script);
 }
 
 ICEntry &
 BaselineScript::icEntry(size_t index)
 {
@@ -444,16 +469,23 @@ BaselineScript::icEntry(size_t index)
 
 PCMappingIndexEntry &
 BaselineScript::pcMappingIndexEntry(size_t index)
 {
     JS_ASSERT(index < numPCMappingIndexEntries());
     return pcMappingIndexEntryList()[index];
 }
 
+BlockCounterEntry &
+BaselineScript::blockCounterEntry(size_t index)
+{
+    JS_ASSERT(index < numBlockCounters());
+    return blockCounterEntryList()[index];
+}
+
 CompactBufferReader
 BaselineScript::pcMappingReader(size_t indexEntry)
 {
     PCMappingIndexEntry &entry = pcMappingIndexEntry(indexEntry);
 
     uint8_t *dataStart = pcMappingData() + entry.bufferOffset;
     uint8_t *dataEnd = (indexEntry == numPCMappingIndexEntries() - 1)
         ? pcMappingData() + pcMappingSize_
@@ -600,16 +632,25 @@ BaselineScript::copyICEntries(HandleScri
         if (realEntry.firstStub()->isTableSwitch()) {
             ICTableSwitch *stub = realEntry.firstStub()->toTableSwitch();
             stub->fixupJumpTable(script, this);
         }
     }
 }
 
 void
+BaselineScript::copyBlockCounterEntries(BlockCounterEntry *entries)
+{
+    for (uint32_t i = 0; i < numBlockCounters(); i++) {
+        BlockCounterEntry &entry = blockCounterEntry(i);
+        entry = entries[i];
+    }
+}
+
+void
 BaselineScript::adoptFallbackStubs(FallbackICStubSpace *stubSpace)
 {
     fallbackStubSpace_.adoptFrom(stubSpace);
 }
 
 void
 BaselineScript::copyPCMappingEntries(const CompactBufferWriter &entries)
 {
@@ -785,16 +826,29 @@ BaselineScript::toggleSPS(bool enable)
         Assembler::ToggleToCmp(pushToggleLocation);
     else
         Assembler::ToggleToJmp(pushToggleLocation);
 #ifdef DEBUG
     spsOn_ = enable;
 #endif
 }
 
+
+void
+BaselineScript::toggleBlockCounters(bool enable)
+{
+    for (size_t i = 0; i < blockCounterEntries_; i++) {
+        CodeLocationLabel counterToggleLocation(method_, blockCounterEntry(i).toggleOffset);
+        if (enable)
+            Assembler::ToggleToCmp(counterToggleLocation);
+        else
+            Assembler::ToggleToJmp(counterToggleLocation);
+    }
+}
+
 void
 BaselineScript::purgeOptimizedStubs(Zone *zone)
 {
     IonSpew(IonSpew_BaselineIC, "Purging optimized stubs");
 
     for (size_t i = 0; i < numICEntries(); i++) {
         ICEntry &entry = icEntry(i);
         if (!entry.hasStub())
diff --git a/js/src/jit/BaselineJIT.h b/js/src/jit/BaselineJIT.h
--- a/js/src/jit/BaselineJIT.h
+++ b/js/src/jit/BaselineJIT.h
@@ -90,16 +90,30 @@ struct PCMappingIndexEntry
 
     // Native code offset.
     uint32_t nativeOffset;
 
     // Offset in the CompactBuffer where data for pcOffset starts.
     uint32_t bufferOffset;
 };
 
+struct BlockCounterEntry
+{
+    uint32_t counter;
+    size_t pcOffset;
+    CodeOffsetLabel toggleOffset;
+    BlockCounterEntry(const size_t pcoffset)
+      : counter(0),
+        pcOffset(pcoffset)
+    { }
+    static size_t offsetOfCounter() {
+        return offsetof(BlockCounterEntry, counter);
+    }
+};
+
 struct BaselineScript
 {
   public:
     static const uint32_t MAX_JSSCRIPT_LENGTH = 0x0fffffffu;
 
   private:
     // Code pointer containing the actual method.
     HeapPtr<IonCode> method_;
@@ -111,16 +125,18 @@ struct BaselineScript
     uint32_t prologueOffset_;
 
     // The offsets for the toggledJump instructions for SPS update ICs.
 #ifdef DEBUG
     mozilla::DebugOnly<bool> spsOn_;
 #endif
     uint32_t spsPushToggleOffset_;
 
+    bool branchProfilingEnabled_;
+
   public:
     enum Flag {
         // Flag set by JSScript::argumentsOptimizationFailed. Similar to
         // JSScript::needsArgsObj_, but can be read from JIT code.
         NEEDS_ARGS_OBJ = 1 << 0,
 
         // Flag set when discarding JIT code, to indicate this script is
         // on the stack and should not be discarded.
@@ -141,24 +157,29 @@ struct BaselineScript
     uint32_t icEntries_;
 
     uint32_t pcMappingIndexOffset_;
     uint32_t pcMappingIndexEntries_;
 
     uint32_t pcMappingOffset_;
     uint32_t pcMappingSize_;
 
+    uint32_t blockCounterOffset_;
+    uint32_t blockCounterEntries_;
+
   public:
     // Do not call directly, use BaselineScript::New. This is public for cx->new_.
     BaselineScript(uint32_t prologueOffset, uint32_t spsPushToggleOffset);
 
     static BaselineScript *New(JSContext *cx, uint32_t prologueOffset,
                                uint32_t spsPushToggleOffset, size_t icEntries,
-                               size_t pcMappingIndexEntries, size_t pcMappingSize);
+                               size_t pcMappingIndexEntries, size_t pcMappingSize,
+                               size_t blockCounters);
     static void Trace(JSTracer *trc, BaselineScript *script);
+    static void DumpBlockCounters(BaselineScript *script);
     static void Destroy(FreeOp *fop, BaselineScript *script);
 
     void purgeOptimizedStubs(Zone *zone);
 
     static inline size_t offsetOfMethod() {
         return offsetof(BaselineScript, method_);
     }
 
@@ -207,16 +228,20 @@ struct BaselineScript
     }
     uint8_t *pcMappingData() {
         return reinterpret_cast<uint8_t *>(this) + pcMappingOffset_;
     }
     FallbackICStubSpace *fallbackStubSpace() {
         return &fallbackStubSpace_;
     }
 
+    BlockCounterEntry *blockCounterEntryList() {
+        return (BlockCounterEntry *)(reinterpret_cast<uint8_t *>(this) + blockCounterOffset_);
+    }
+
     IonCode *method() const {
         return method_;
     }
     void setMethod(IonCode *code) {
         JS_ASSERT(!method_);
         method_ = code;
     }
 
@@ -238,20 +263,27 @@ struct BaselineScript
     }
 
     void copyICEntries(HandleScript script, const ICEntry *entries, MacroAssembler &masm);
     void adoptFallbackStubs(FallbackICStubSpace *stubSpace);
 
     PCMappingIndexEntry &pcMappingIndexEntry(size_t index);
     CompactBufferReader pcMappingReader(size_t indexEntry);
 
+    BlockCounterEntry &blockCounterEntry(size_t index);
+    void copyBlockCounterEntries(BlockCounterEntry *entries);
+
     size_t numPCMappingIndexEntries() const {
         return pcMappingIndexEntries_;
     }
 
+    size_t numBlockCounters() const {
+        return blockCounterEntries_;
+    }
+
     void copyPCMappingIndexEntries(const PCMappingIndexEntry *entries);
 
     void copyPCMappingEntries(const CompactBufferWriter &entries);
     uint8_t *nativeCodeForPC(JSScript *script, jsbytecode *pc, PCMappingSlotInfo *slotInfo = NULL);
     jsbytecode *pcForReturnOffset(JSScript *script, uint32_t nativeOffset);
     jsbytecode *pcForReturnAddress(JSScript *script, uint8_t *nativeAddress);
 
     // Toggle debug traps (used for breakpoints and step mode) in the script.
@@ -264,16 +296,34 @@ struct BaselineScript
     void noteAccessedGetter(uint32_t pcOffset);
     void noteArrayWriteHole(uint32_t pcOffset);
 
     static size_t offsetOfFlags() {
         return offsetof(BaselineScript, flags_);
     }
 
     static void writeBarrierPre(Zone *zone, BaselineScript *script);
+
+    void toggleBlockCounters(bool enable);
+
+    bool isBranchProfilingEnabled() {
+        return branchProfilingEnabled_;
+    }
+
+    void enableBranchProfiling() {
+        JS_ASSERT(!branchProfilingEnabled_);
+        branchProfilingEnabled_ = true;
+        toggleBlockCounters(branchProfilingEnabled_);
+    }
+
+    void disableBranchProfiling() {
+        JS_ASSERT(branchProfilingEnabled_);
+        branchProfilingEnabled_ = false;
+        toggleBlockCounters(branchProfilingEnabled_);
+    }
 };
 
 inline bool
 IsBaselineEnabled(JSContext *cx)
 {
     return cx->hasOption(JSOPTION_BASELINE);
 }
 
diff --git a/js/src/jit/Ion.cpp b/js/src/jit/Ion.cpp
--- a/js/src/jit/Ion.cpp
+++ b/js/src/jit/Ion.cpp
@@ -1181,16 +1181,26 @@ OptimizeMIR(MIRGenerator *mir)
 
     if (mir->shouldCancel("Renumber Blocks"))
         return false;
 
     if (!BuildDominatorTree(graph))
         return false;
     // No spew: graph not changed.
 
+    if (js_IonOptions.baselineBranchProfiling && !mir->compilingAsmJS()) {
+        if (!AttachBranchProfiles(mir, graph))
+            return false;
+        IonSpewPass("Attach Branch Profiles");
+        AssertGraphCoherency(graph);
+
+        if (mir->shouldCancel("Attach Branch Profiles"))
+            return false;
+    }
+
     if (mir->shouldCancel("Dominator Tree"))
         return false;
 
     // Aggressive phi elimination must occur before any code elimination. If the
     // script contains a try-statement, we only compiled the try block and not
     // the catch or finally blocks, so in this case it's also invalid to use
     // aggressive phi elimination.
     Observability observability = graph.hasTryBlock()
@@ -1575,16 +1585,21 @@ IonCompile(JSContext *cx, JSScript *scri
 {
 #if JS_TRACE_LOGGING
     AutoTraceLog logger(TraceLogging::defaultLogger(),
                         TraceLogging::ION_COMPILE_START,
                         TraceLogging::ION_COMPILE_STOP,
                         script);
 #endif
 
+#ifdef DEBUG
+    if (js_IonOptions.baselineBranchProfiling && script->hasBaselineScript())
+        BaselineScript::DumpBlockCounters(script->baselineScript());
+#endif
+
     LifoAlloc *alloc = cx->new_<LifoAlloc>(BUILDER_LIFO_ALLOC_PRIMARY_CHUNK_SIZE);
     if (!alloc)
         return AbortReason_Alloc;
 
     ScopedJSDeletePtr<LifoAlloc> autoDelete(alloc);
 
     TempAllocator *temp = alloc->new_<TempAllocator>(alloc);
     if (!temp)
@@ -2519,16 +2534,22 @@ jit::ForbidCompilation(JSContext *cx, JS
             // running, because IonFrameIterator needs to tell what ionScript to
             // use (either the one on the JSScript, or the one hidden in the
             // breadcrumbs Invalidation() leaves). Therefore, if invalidation
             // fails, we cannot disable the script.
             if (!Invalidate(cx, script, mode, false))
                 return;
         }
 
+        // NOTE: Scripts in bug757428.js entered ForbidCompilation() more than once
+        // with --baseline-eager option, which crashed toggleBlockCounters().
+        // To void this we test the state of block counters before we toggle jumps.
+        if (script->hasBaselineScript() && script->baselineScript()->isBranchProfilingEnabled())
+            script->baselineScript()->disableBranchProfiling();
+
         script->setIonScript(ION_DISABLED_SCRIPT);
         return;
 
       case ParallelExecution:
         if (script->hasParallelIonScript()) {
             if (!Invalidate(cx, script, mode, false))
                 return;
         }
diff --git a/js/src/jit/Ion.h b/js/src/jit/Ion.h
--- a/js/src/jit/Ion.h
+++ b/js/src/jit/Ion.h
@@ -192,16 +192,21 @@ struct IonOptions
     // Default: false
     bool eagerCompilation;
 
     // How many uses of a parallel kernel before we attempt compilation.
     //
     // Default: 1
     uint32_t usesBeforeCompilePar;
 
+    // Whether baseline scripts are instrumented.
+    //
+    // Default: false
+    bool baselineBranchProfiling;
+   
     void setEagerCompilation() {
         eagerCompilation = true;
         usesBeforeCompile = 0;
         baselineUsesBeforeCompile = 0;
     }
 
     IonOptions()
       : gvn(true),
@@ -229,17 +234,18 @@ struct IonOptions
         maxStackArgs(4096),
         maxInlineDepth(3),
         smallFunctionMaxInlineDepth(10),
         smallFunctionMaxBytecodeLength(100),
         polyInlineMax(4),
         inlineMaxTotalBytecodeLength(1000),
         inlineUseCountRatio(128),
         eagerCompilation(false),
-        usesBeforeCompilePar(1)
+        usesBeforeCompilePar(1),
+        baselineBranchProfiling(true)
     {
     }
 
     uint32_t usesBeforeInlining() {
         return usesBeforeCompile * usesBeforeInliningFactor;
     }
 };
 
diff --git a/js/src/jit/IonAnalysis.cpp b/js/src/jit/IonAnalysis.cpp
--- a/js/src/jit/IonAnalysis.cpp
+++ b/js/src/jit/IonAnalysis.cpp
@@ -4,18 +4,20 @@
  * License, v. 2.0. If a copy of the MPL was not distributed with this
  * file, You can obtain one at http://mozilla.org/MPL/2.0/. */
 
 #include "jit/IonAnalysis.h"
 
 #include "jsanalyze.h"
 
 #include "jit/BaselineInspector.h"
+#include "jit/BaselineJIT.h"
 #include "jit/Ion.h"
 #include "jit/IonBuilder.h"
+#include "jit/IonSpewer.h"
 #include "jit/LIR.h"
 #include "jit/Lowering.h"
 #include "jit/MIRGraph.h"
 
 #include "jsinferinlines.h"
 
 using namespace js;
 using namespace js::jit;
@@ -44,16 +46,249 @@ jit::SplitCriticalEdges(MIRGraph &graph)
 
             block->replaceSuccessor(i, split);
             target->replacePredecessor(*block, split);
         }
     }
     return true;
 }
 
+bool
+jit::AttachBranchProfiles(MIRGenerator *mir, MIRGraph &graph)
+{
+    // Forbid AsmJS optimization as OdinMonkey is an ahead of time compiler
+    // and we do not have any profiled information.
+    JS_ASSERT(!mir->compilingAsmJS());
+
+    Vector<MBasicBlock *, 0, SystemAllocPolicy> splitEdges;
+    for (ReversePostorderIterator block(graph.rpoBegin()); block != graph.rpoEnd(); block++) {
+        CompileInfo &info = block->info();
+        jsbytecode *code = info.startPC();
+        JSScript *jsscript = info.script();
+        if (!jsscript || !jsscript->hasBaselineScript())
+            continue;
+
+        // All MBasicBlocks constructed by IonBuilder should have a valid pc,
+        // except AsmJS scripts.
+        jsbytecode *pc = block->pc();
+        JS_ASSERT(pc);
+
+        IonSpew(IonSpew_BranchProfiles,
+                "AttachBranchProfiles finding counters for block %d (%p) pcOffset = %d",
+                block->id(), jsscript, pc - code);
+
+        // BlockUseCount of the basic blocks generated by SplitCriticalEdges()
+        // should be determinated by its siblings and the predecessor of it.
+        // We skip split blocks since they share the same pc of its predecessor,
+        // which might give them wrong profiles.
+        if (block->isSplitEdge()) {
+            if (!splitEdges.append(*block))
+                return false;
+            continue;
+        }
+
+        // OSR block is generated for on stack replacement, which has no related
+        // counters. Although a counter might be found for the OSR block (because it
+        // copies the pc from the basic block it enters), its blockUseCount should
+        // always be 1.
+        if (*block == graph.osrBlock()) {
+            block->setBlockUseCount(1);
+            continue;
+        }
+
+        // In baseline compiler we instrument 'JSOP_LOOPHEAD', while IonBuilder
+        // might assign the pc of previous opcode (JSOP_NOP or JSOP_GOTO)
+        // to corresponding MBasicBlock. These mismatch can be corrected
+        // by adding the length of the previous opcode, if the opcode is JSOP_NOP,
+        // or using the counter for the pc that the JSOP_GOTO jumps to.
+        JSOp op = JSOp(*pc);
+        if (block->isLoopHeader()) {
+            if (op == JSOP_GOTO) {
+                pc += GET_JUMP_OFFSET(pc);
+                JS_ASSERT(JSOp(*pc) == JSOP_LOOPENTRY);
+            } else {
+                if (op == JSOP_NOP)
+                    pc += GetBytecodeLength(pc);
+                JS_ASSERT(JSOp(*pc) == JSOP_LOOPHEAD);
+            }
+        } else {
+            // FIXME: Temporary hacks for bug736135.js
+            // In bug736135.js, the second for loop in function test() does not
+            // has "isLoopHeader()" flag, since there is a 'return' in its loop body.
+            // In this case we inspect the opcode to choose the right counter.
+            if (op == JSOP_GOTO) {
+                jsbytecode *nextpc = pc + GetBytecodeLength(pc);
+                jsbytecode *targetpc = pc + GET_JUMP_OFFSET(pc);
+                if (JSOp(*nextpc) == JSOP_LOOPHEAD && JSOp(*targetpc) == JSOP_LOOPENTRY)
+                pc += GET_JUMP_OFFSET(pc);
+            }
+        }
+
+        // Since the value of numCounters is usually less than 10,
+        // the linear search algorithm is fast enough.
+        // We can switch to binary search if necessary.
+        BaselineScript *blscript = jsscript->baselineScript();
+        size_t numCounters = blscript->numBlockCounters();
+        size_t pcOffset = pc - code;
+        for (size_t i = 0; i < numCounters; i++) {
+            BlockCounterEntry entry = blscript->blockCounterEntry(i);
+            if (entry.pcOffset == pcOffset) {
+                block->setBlockUseCount(entry.counter);
+                IonSpew(IonSpew_BranchProfiles,
+                        "AttachBranchProfiles match counter %d (%p)-> block %d (pc %d), value = %d",
+                        i, jsscript, block->id(), pcOffset, entry.counter);
+                break;
+            }
+        }
+
+        // Two blocks are generated for JSOP_TRY during MIRGraph building
+        // with different bytecode addresses (pc). The pc of the first block
+        // is the address of JSOP_TRY, which has a counter available.
+        // The pc of second block is the address of the bytecode right after
+        // the JSOP_TRY, which is not a jump target. In this case we copy the
+        // counter of JSOP_TRY to the second block.
+        //
+        // code example:
+        //     jit-test/tests/auto-regress/bug655950.js
+        if (!block->isBlockUseCountAvailable() && block->numPredecessors() == 1) {
+            MBasicBlock *pred = block->getPredecessor(0);
+            JSOp op = JSOp(*(pred->pc()));
+            if (op == JSOP_TRY) {
+                JS_ASSERT(pred->pc() + GetBytecodeLength(pred->pc()) == block->pc());
+                JS_ASSERT(pred->isBlockUseCountAvailable());
+                block->setBlockUseCount(pred->getBlockUseCount());
+                IonSpew(IonSpew_BranchProfiles,
+                        "AttachBranchProfiles match JSOP_TRY block %d -> block %d, value = %d",
+                        pred->id(), block->id(), block->getBlockUseCount());
+            }
+        }
+
+        // When a function call is inlined, the basic block which calls
+        // this function will be divided into two blocks, namely A and B,
+        // and block B will have a new pc, which has no counter available.
+        // In this case, we copy the counter from the first dominator
+        // of the block which come from the same script (It should be A).
+        //
+        // We should not use MBasicBlock::info()::script() to distinguish
+        // whether a block is inlined or not, for a function might be inlined
+        // by the function itself (i.e. recursive call).
+        //
+        // Code example:
+        //     jit-test/tests/jaeger/fused-eq-ifeq.js
+        if (!block->isBlockUseCountAvailable() && block->numPredecessors()) {
+            MResumePoint *resumePoint = block->callerResumePoint();
+            size_t i = 0;
+            // If a basic block is generated during function inlining, then all
+            // its predecessors should come from inlined functions. Otherwise
+            // we should not copy the counter from its dominator.
+            for (; i < block->numPredecessors(); i++)
+                if (resumePoint == block->getPredecessor(i)->callerResumePoint())
+                    break;
+            if (i == block->numPredecessors()) {
+                MBasicBlock *dom = block->immediateDominator();
+                JS_ASSERT(dom && dom != *block);
+                while (dom && resumePoint != dom->callerResumePoint()) {
+                    JS_ASSERT(dom && dom != dom->immediateDominator());
+                    dom = dom->immediateDominator();
+                }
+                if (dom->isBlockUseCountAvailable()) {
+                    block->setBlockUseCount(dom->getBlockUseCount());
+                    IonSpew(IonSpew_BranchProfiles,
+                            "AttachBranchProfiles inline copy pred %d -> block %d, value = %d",
+                            dom->id(), block->id(), block->getBlockUseCount());
+                } else {
+                    IonSpew(IonSpew_BranchProfiles,
+                            "AttachBranchProfiles inline MISSING block %d, pc = %d",
+                            block->id(), pcOffset);
+                }
+            }
+        }
+        // 'FunctionDispatch' and 'TypeObjectDispatch' generates multiple
+        // basic blocks before and after inlined functions, and a return block
+        // as their post dominator. This return block has no counter available.
+        // In this case, it should use its dominator's counter.
+        //
+        // Code example:
+        //     jit-test/tests/ion/bug852342.js
+        if (!block->isBlockUseCountAvailable()) {
+            MBasicBlock *dom = block->immediateDominator();
+            // Skip self-dominated blocks.
+            while (dom && dom != dom->immediateDominator()) {
+                IonSpew(IonSpew_BranchProfiles, "AttachBranchProfiles while dispatch %d", dom->id());
+                if (dom->callerResumePoint() == block->callerResumePoint()) {
+                    MControlInstruction *lastIns = dom->lastIns();
+                    JS_ASSERT(lastIns);
+                    if (lastIns->isFunctionDispatch() || lastIns->isTypeObjectDispatch()) {
+                        if (dom->isBlockUseCountAvailable()) {
+                            block->setBlockUseCount(dom->getBlockUseCount());
+                            IonSpew(IonSpew_BranchProfiles,
+                                    "AttachBranchProfiles DISPATCH id %d (pc %d)-> dom %d (pc %d)",
+                                    jsscript, block->id(), pcOffset, dom->id(), dom->pc() - code);
+                        } else {
+                            IonSpew(IonSpew_BranchProfiles,
+                                    "AttachBranchProfiles DISPATCH id %d (pc %d)-> dom %d (pc %d) FAIL",
+                                    jsscript, block->id(), pcOffset, dom->id(), dom->pc() - pc);
+                        }
+                        break;
+                    }
+                }
+                dom = dom->immediateDominator();
+            }
+        }
+
+        // If all previous heuristics are failed, simply copy its dominator's profile.
+        if (!block->isBlockUseCountAvailable()) {
+            MBasicBlock *dom = block->immediateDominator();
+            if (dom && dom->isBlockUseCountAvailable())
+                block->setBlockUseCount(dom->getBlockUseCount());
+        }
+    }
+
+    // Calculating the execution count for split blocks. The result might be
+    // imprecise, since it is calculated from its predecessor and siblings only.
+    // Currently the result is sufficient for subsequent optimizations, and
+    // we can change to more accurate algorithms if needed.
+    while (splitEdges.length()) {
+        MBasicBlock *splitBlock = splitEdges.popCopy();
+        JS_ASSERT(splitBlock->numPredecessors() == 1);
+        MBasicBlock *pred = splitBlock->getPredecessor(0);
+        JS_ASSERT(pred->isBlockUseCountAvailable());
+        uint32_t predUseCount = pred->getBlockUseCount();
+        uint32_t sum = 0;
+        for (size_t i = 0; i < pred->numSuccessors(); i++) {
+            MBasicBlock *succ = pred->getSuccessor(i);
+            // 'TableSwitch' and 'CondSwitch' may generate multiple split blocks.
+            // Skip all of them, including current block itself.
+            if (succ->isSplitEdge())
+                continue;
+            if (succ->isBlockUseCountAvailable())
+                sum += succ->getBlockUseCount();
+        }
+        IonSpew(IonSpew_BranchProfiles,
+                "AttachBranchProfiles splitEdges jsscript(%p) : block %d, pred %u - sum %u = %d",
+                splitBlock->info().script(), splitBlock->id(), pred->getBlockUseCount(),
+                sum, predUseCount - sum);
+        if (sum <= predUseCount)
+            splitBlock->setBlockUseCount(predUseCount - sum);
+        else
+            splitBlock->setBlockUseCount(predUseCount / 2);
+    }
+
+#ifdef DEBUG
+    for (ReversePostorderIterator block(graph.rpoBegin()); block != graph.rpoEnd(); block++) {
+        if (!block->isBlockUseCountAvailable()) {
+            IonSpew(IonSpew_BranchProfiles,
+                    "AttachBranchProfiles mismatch jsscript(%p) -> block %d",
+                    block->info().script(), block->id(), block->pc() - block->info().startPC());
+        }
+    }
+#endif
+    return true;
+}
+
 // Operands to a resume point which are dead at the point of the resume can be
 // replaced with undefined values. This analysis supports limited detection of
 // dead operands, pruning those which are defined in the resume point's basic
 // block and have no uses outside the block or at points later than the resume
 // point.
 //
 // This is intended to ensure that extra resume points within a basic block
 // will not artificially extend the lifetimes of any SSA values. This could
diff --git a/js/src/jit/IonAnalysis.h b/js/src/jit/IonAnalysis.h
--- a/js/src/jit/IonAnalysis.h
+++ b/js/src/jit/IonAnalysis.h
@@ -16,16 +16,19 @@ namespace js {
 namespace jit {
 
 class MIRGenerator;
 class MIRGraph;
 
 bool
 SplitCriticalEdges(MIRGraph &graph);
 
+bool
+AttachBranchProfiles(MIRGenerator *mir, MIRGraph &graph);
+
 enum Observability {
     ConservativeObservability,
     AggressiveObservability
 };
 
 bool
 EliminatePhis(MIRGenerator *mir, MIRGraph &graph, Observability observe);
 
diff --git a/js/src/jit/IonSpewer.cpp b/js/src/jit/IonSpewer.cpp
--- a/js/src/jit/IonSpewer.cpp
+++ b/js/src/jit/IonSpewer.cpp
@@ -257,16 +257,18 @@ jit::CheckLogging()
             "  caches     Inline caches\n"
             "  osi        Invalidation\n"
             "  safepoints Safepoints\n"
             "  pools      Literal Pools (ARM only for now)\n"
             "  cacheflush Instruction Cache flushes (ARM only for now)\n"
             "  range      Range Analysis\n"
             "  logs       C1 and JSON visualization logging\n"
             "  trace      Generate calls to js::jit::Trace() for effectful instructions\n"
+            "  branchprofiles\n"
+            "             Dump branch profiling data\n"
             "  all        Everything\n"
             "\n"
             "  bl-aborts  Baseline compiler abort messages\n"
             "  bl-scripts Baseline script-compilation\n"
             "  bl-op      Baseline compiler detailed op-specific messages\n"
             "  bl-ic      Baseline inline-cache messages\n"
             "  bl-ic-fb   Baseline IC fallback stub messages\n"
             "  bl-osr     Baseline IC OSR messages\n"
@@ -310,16 +312,18 @@ jit::CheckLogging()
     if (ContainsFlag(env, "pools"))
         EnableChannel(IonSpew_Pools);
     if (ContainsFlag(env, "cacheflush"))
         EnableChannel(IonSpew_CacheFlush);
     if (ContainsFlag(env, "logs"))
         EnableIonDebugLogging();
     if (ContainsFlag(env, "trace"))
         EnableChannel(IonSpew_Trace);
+    if (ContainsFlag(env, "branchprofiles"))
+        EnableChannel(IonSpew_BranchProfiles);
     if (ContainsFlag(env, "all"))
         LoggingBits = uint32_t(-1);
 
     if (ContainsFlag(env, "bl-aborts"))
         EnableChannel(IonSpew_BaselineAbort);
     if (ContainsFlag(env, "bl-scripts"))
         EnableChannel(IonSpew_BaselineScripts);
     if (ContainsFlag(env, "bl-op"))
diff --git a/js/src/jit/IonSpewer.h b/js/src/jit/IonSpewer.h
--- a/js/src/jit/IonSpewer.h
+++ b/js/src/jit/IonSpewer.h
@@ -52,16 +52,18 @@ namespace jit {
     /* Debug info about safepoints */       \
     _(Safepoints)                           \
     /* Debug info about Pools*/             \
     _(Pools)                                \
     /* Calls to js::jit::Trace() */         \
     _(Trace)                                \
     /* Debug info about the I$ */           \
     _(CacheFlush)                           \
+    /* Branch Profiling */                  \
+    _(BranchProfiles)                       \
                                             \
     /* BASELINE COMPILER SPEW */            \
                                             \
     /* Aborting Script Compilation. */      \
     _(BaselineAbort)                        \
     /* Script Compilation. */               \
     _(BaselineScripts)                      \
     /* Detailed op-specific spew. */        \
diff --git a/js/src/jit/JSONSpewer.cpp b/js/src/jit/JSONSpewer.cpp
--- a/js/src/jit/JSONSpewer.cpp
+++ b/js/src/jit/JSONSpewer.cpp
@@ -292,16 +292,19 @@ JSONSpewer::spewMIR(MIRGraph *mir)
     beginObjectProperty("mir");
     beginListProperty("blocks");
 
     for (MBasicBlockIterator block(mir->begin()); block != mir->end(); block++) {
         beginObject();
 
         integerProperty("number", block->id());
 
+        if(block->isBlockUseCountAvailable())
+            integerProperty("blockUseCount", block->getBlockUseCount());
+
         beginListProperty("attributes");
         if (block->isLoopBackedge())
             stringValue("backedge");
         if (block->isLoopHeader())
             stringValue("loopheader");
         if (block->isSplitEdge())
             stringValue("splitedge");
         endList();
@@ -367,16 +370,19 @@ JSONSpewer::spewLIR(MIRGraph *mir)
     for (MBasicBlockIterator i(mir->begin()); i != mir->end(); i++) {
         LBlock *block = i->lir();
         if (!block)
             continue;
 
         beginObject();
         integerProperty("number", i->id());
 
+        if (i->isBlockUseCountAvailable())
+            integerProperty("blockUseCount", i->getBlockUseCount());
+
         beginListProperty("instructions");
         for (size_t p = 0; p < block->numPhis(); p++)
             spewLIns(block->getPhi(p));
         for (LInstructionIterator ins(block->begin()); ins != block->end(); ins++)
             spewLIns(*ins);
         endList();
 
         endObject();
@@ -393,19 +399,27 @@ JSONSpewer::spewIntervals(LinearScanAllo
         return;
 
     beginObjectProperty("intervals");
     beginListProperty("blocks");
 
     for (size_t bno = 0; bno < regalloc->graph.numBlocks(); bno++) {
         beginObject();
         integerProperty("number", bno);
+
+        LBlock *lir = regalloc->graph.getBlock(bno);
+
+        // Currently we don't propagate blockUseCounts to LBlock,
+        // so we use the counter of the corresponding MBasicBlock instead.
+        MBasicBlock *mir = lir->mir();
+        if (mir && mir->isBlockUseCountAvailable())
+            integerProperty("blockUseCount", mir->getBlockUseCount());
+
         beginListProperty("vregs");
 
-        LBlock *lir = regalloc->graph.getBlock(bno);
         for (LInstructionIterator ins = lir->begin(); ins != lir->end(); ins++) {
             for (size_t k = 0; k < ins->numDefs(); k++) {
                 VirtualRegister *vreg = &regalloc->vregs[ins->getDef(k)->virtualRegister()];
 
                 beginObject();
                 integerProperty("vreg", vreg->id());
                 beginListProperty("intervals");
 
diff --git a/js/src/jit/MIRGraph.cpp b/js/src/jit/MIRGraph.cpp
--- a/js/src/jit/MIRGraph.cpp
+++ b/js/src/jit/MIRGraph.cpp
@@ -288,17 +288,19 @@ MBasicBlock::MBasicBlock(MIRGraph &graph
     successorWithPhis_(NULL),
     positionInPhiSuccessor_(0),
     kind_(kind),
     loopDepth_(0),
     mark_(false),
     immediateDominator_(NULL),
     numDominated_(0),
     loopHeader_(NULL),
-    trackedPc_(pc)
+    trackedPc_(pc),
+    blockUseCount_(0),
+    blockUseCountAvailable_(false)
 #if defined (JS_ION_PERF)
     , lineno_(0u),
     columnIndex_(0u)
 #endif
 {
 }
 
 bool
diff --git a/js/src/jit/MIRGraph.h b/js/src/jit/MIRGraph.h
--- a/js/src/jit/MIRGraph.h
+++ b/js/src/jit/MIRGraph.h
@@ -481,16 +481,31 @@ class MBasicBlock : public TempObject, p
     void updateTrackedPc(jsbytecode *pc) {
         trackedPc_ = pc;
     }
 
     jsbytecode *trackedPc() {
         return trackedPc_;
     }
 
+    // SplitCriticalEdges() and function inlining may introduce new MBasicBlocks,
+    // which has no profile available.
+    bool isBlockUseCountAvailable() {
+        return blockUseCountAvailable_;
+    }
+
+    void setBlockUseCount(const uint32_t useCount) {
+        blockUseCount_ = useCount;
+        blockUseCountAvailable_ = true;
+    }
+
+    uint32_t getBlockUseCount() {
+        return blockUseCount_;
+    }
+
   private:
     MIRGraph &graph_;
     CompileInfo &info_; // Each block originates from a particular script.
     InlineList<MInstruction> instructions_;
     Vector<MBasicBlock *, 1, IonAllocPolicy> predecessors_;
     InlineForwardList<MPhi> phis_;
     InlineForwardList<MResumePoint> resumePoints_;
     FixedList<MDefinition *> slots_;
@@ -501,16 +516,18 @@ class MBasicBlock : public TempObject, p
     uint32_t domIndex_; // Index in the dominator tree.
     LBlock *lir_;
     MStart *start_;
     MResumePoint *entryResumePoint_;
     MBasicBlock *successorWithPhis_;
     uint32_t positionInPhiSuccessor_;
     Kind kind_;
     uint32_t loopDepth_;
+    uint32_t blockUseCount_;
+    bool blockUseCountAvailable_;
 
     // Utility mark for traversal algorithms.
     bool mark_;
 
     Vector<MBasicBlock *, 1, IonAllocPolicy> immediatelyDominated_;
     MBasicBlock *immediateDominator_;
     size_t numDominated_;
     MBasicBlock *loopHeader_;
diff --git a/js/src/jit/shared/BaselineCompiler-shared.h b/js/src/jit/shared/BaselineCompiler-shared.h
--- a/js/src/jit/shared/BaselineCompiler-shared.h
+++ b/js/src/jit/shared/BaselineCompiler-shared.h
@@ -27,16 +27,17 @@ class BaselineCompilerShared
     bool ionOSRCompileable_;
     bool debugMode_;
 
     BytecodeAnalysis analysis_;
     FrameInfo frame;
 
     FallbackICStubSpace stubSpace_;
     js::Vector<ICEntry, 16, SystemAllocPolicy> icEntries_;
+    js::Vector<BlockCounterEntry, 16, SystemAllocPolicy> blockCounterEntries_;
 
     // Stores the native code offset for a bytecode pc.
     struct PCMappingEntry
     {
         uint32_t pcOffset;
         uint32_t nativeOffset;
         PCMappingSlotInfo slotInfo;
 
@@ -59,16 +60,22 @@ class BaselineCompilerShared
     // to be patched with the actual icEntry offsets after the BaselineScript
     // has been allocated.
     struct ICLoadLabel {
         size_t icEntry;
         CodeOffsetLabel label;
     };
     js::Vector<ICLoadLabel, 16, SystemAllocPolicy> icLoadLabels_;
 
+    struct BlockCounterLabel {
+        size_t bcEntry;
+        CodeOffsetLabel label;
+    };
+    js::Vector<BlockCounterLabel, 16, SystemAllocPolicy> blockCounterLabels_;
+
     uint32_t pushedBeforeCall_;
     mozilla::DebugOnly<bool> inCall_;
 
     CodeOffsetLabel spsPushToggleOffset_;
 
     BaselineCompilerShared(JSContext *cx, HandleScript script);
 
     ICEntry *allocateICEntry(ICStub *stub, bool isForOp) {
@@ -90,16 +97,30 @@ class BaselineCompilerShared
     bool addICLoadLabel(CodeOffsetLabel label) {
         JS_ASSERT(!icEntries_.empty());
         ICLoadLabel loadLabel;
         loadLabel.label = label;
         loadLabel.icEntry = icEntries_.length() - 1;
         return icLoadLabels_.append(loadLabel);
     }
 
+    BlockCounterEntry *allocateBlockCounterEntry(const size_t pcoffset) {
+        if(!blockCounterEntries_.append(BlockCounterEntry(pcoffset)))
+            return NULL;
+        return &blockCounterEntries_.back();
+    }
+
+    bool addBlockCounterLabel(CodeOffsetLabel label) {
+        JS_ASSERT(!blockCounterEntries_.empty());
+        BlockCounterLabel bcLabel;
+        bcLabel.label = label;
+        bcLabel.bcEntry = blockCounterEntries_.length() - 1;
+        return blockCounterLabels_.append(bcLabel);
+    }
+
     JSFunction *function() const {
         return script->function();
     }
 
     PCMappingSlotInfo getStackTopSlotInfo() {
         JS_ASSERT(frame.numUnsyncedSlots() <= 2);
         switch (frame.numUnsyncedSlots()) {
           case 0:
diff --git a/js/src/shell/js.cpp b/js/src/shell/js.cpp
--- a/js/src/shell/js.cpp
+++ b/js/src/shell/js.cpp
@@ -5020,16 +5020,25 @@ ProcessArgs(JSContext *cx, JSObject *obj
 
     useCount = op->getIntOption("baseline-uses-before-compile");
     if (useCount >= 0)
         jit::js_IonOptions.baselineUsesBeforeCompile = useCount;
 
     if (op->getBoolOption("baseline-eager"))
         jit::js_IonOptions.baselineUsesBeforeCompile = 0;
 
+    if (const char *str = op->getStringOption("branch-profiling")) {
+        if (strcmp(str, "off") == 0)
+            jit::js_IonOptions.baselineBranchProfiling = false;
+        else if (strcmp(str, "on") == 0)
+            jit::js_IonOptions.baselineBranchProfiling = true;
+        else
+            return OptionFailure("branch-profiling", str);
+    }
+
     if (const char *str = op->getStringOption("ion-regalloc")) {
         if (strcmp(str, "lsra") == 0)
             jit::js_IonOptions.registerAllocator = jit::RegisterAllocator_LSRA;
         else if (strcmp(str, "backtracking") == 0)
             jit::js_IonOptions.registerAllocator = jit::RegisterAllocator_Backtracking;
         else if (strcmp(str, "stupid") == 0)
             jit::js_IonOptions.registerAllocator = jit::RegisterAllocator_Stupid;
         else
@@ -5287,16 +5296,18 @@ main(int argc, char **argv, char **envp)
         || !op.addIntOption('\0', "ion-uses-before-compile", "COUNT",
                             "Wait for COUNT calls or iterations before compiling "
                             "(default: 1000)", -1)
         || !op.addStringOption('\0', "ion-regalloc", "[mode]",
                                "Specify Ion register allocation:\n"
                                "  lsra: Linear Scan register allocation (default)\n"
                                "  backtracking: Priority based backtracking register allocation\n"
                                "  stupid: Simple block local register allocation")
+        || !op.addStringOption('\0', "branch-profiling", "on/off",
+                               "Profile baseline generated codes (default: off, on to enable)")
         || !op.addBoolOption('\0', "ion-eager", "Always ion-compile methods (implies --baseline-eager)")
         || !op.addBoolOption('\0', "ion-compile-try-catch", "Ion-compile try-catch statements")
 #ifdef JS_THREADSAFE
         || !op.addStringOption('\0', "ion-parallel-compile", "on/off",
                                "Compile scripts off thread (default: off)")
 #endif
         || !op.addBoolOption('\0', "baseline", "Enable baseline compiler (default)")
         || !op.addBoolOption('\0', "no-baseline", "Disable baseline compiler")
