# HG changeset patch
# Parent 0209498700fd33ef61aeb094932425cfd01c3ef1
# User Wei Wu <lazyparser@gmail.com>

diff --git a/js/src/jit/CodeGenerator.cpp b/js/src/jit/CodeGenerator.cpp
--- a/js/src/jit/CodeGenerator.cpp
+++ b/js/src/jit/CodeGenerator.cpp
@@ -911,17 +911,18 @@ CodeGenerator::visitReturn(LReturn *lir)
     DebugOnly<LAllocation *> payload = lir->getOperand(PAYLOAD_INDEX);
     JS_ASSERT(ToRegister(type)    == JSReturnReg_Type);
     JS_ASSERT(ToRegister(payload) == JSReturnReg_Data);
 #elif defined(JS_PUNBOX64)
     DebugOnly<LAllocation *> result = lir->getOperand(0);
     JS_ASSERT(ToRegister(result) == JSReturnReg);
 #endif
     // Don't emit a jump to the return label if this is the last block.
-    if (current->mir() != *gen->graph().poBegin())
+    // if (current->mir() != *gen->graph().poBegin())
+    if (current != graph.getBlock(graph.numBlocks() - 1))
         masm.jump(&returnLabel_);
     return true;
 }
 
 bool
 CodeGenerator::visitOsrEntry(LOsrEntry *lir)
 {
     // Remember the OSR entry offset into the code buffer.
@@ -963,28 +964,40 @@ CodeGenerator::visitStackArgT(LStackArgT
 
     if (arg->isFloatReg())
         masm.storeDouble(ToFloatRegister(arg), dest);
     else if (arg->isRegister())
         masm.storeValue(ValueTypeFromMIRType(argType), ToRegister(arg), dest);
     else
         masm.storeValue(*(arg->toConstant()), dest);
 
-    return pushedArgumentSlots_.append(StackOffsetToSlot(stack_offset));
+    bool ret = pushedArgumentSlots_.append(StackOffsetToSlot(stack_offset));
+    IonSpew(IonSpew_BranchProfiles,
+            "CodeGenerator::visitStackArgT(LStackArgT *lir), argslot=%zu, stack_offset=%d, pushedArgumentSlots_.length()=%zu",
+            argslot,
+            stack_offset,
+            pushedArgumentSlots_.length());
+    return ret;
 }
 
 bool
 CodeGenerator::visitStackArgV(LStackArgV *lir)
 {
     ValueOperand val = ToValue(lir, 0);
     uint32_t argslot = lir->argslot();
     int32_t stack_offset = StackOffsetOfPassedArg(argslot);
 
     masm.storeValue(val, Address(StackPointer, stack_offset));
-    return pushedArgumentSlots_.append(StackOffsetToSlot(stack_offset));
+    bool ret = pushedArgumentSlots_.append(StackOffsetToSlot(stack_offset));
+    IonSpew(IonSpew_BranchProfiles,
+            "CodeGenerator::visitStackArgT(LStackArgT *lir), argslot=%zu, stack_offset=%d, pushedArgumentSlots_.length()=%zu",
+            argslot,
+            stack_offset,
+            pushedArgumentSlots_.length());
+    return ret;
 }
 
 bool
 CodeGenerator::visitMoveGroup(LMoveGroup *group)
 {
     if (!group->numMoves())
         return true;
 
@@ -1434,16 +1447,20 @@ CodeGenerator::visitCallNative(LCallNati
 {
     JSFunction *target = call->getSingleTarget();
     JS_ASSERT(target);
     JS_ASSERT(target->isNative());
 
     int callargslot = call->argslot();
     int unusedStack = StackOffsetOfPassedArg(callargslot);
 
+    IonSpew(IonSpew_BranchProfiles,
+            "CodeGenerator::visitCallNative(LCallNative *call), callargslot=%d, unusedStack=%d",
+            callargslot,
+            unusedStack);
     // Registers used for callWithABI() argument-passing.
     const Register argContextReg   = ToRegister(call->getArgContextReg());
     const Register argUintNReg     = ToRegister(call->getArgUintNReg());
     const Register argVpReg        = ToRegister(call->getArgVpReg());
 
     // Misc. temporary registers.
     const Register tempReg = ToRegister(call->getTempReg());
 
diff --git a/js/src/jit/Ion.cpp b/js/src/jit/Ion.cpp
--- a/js/src/jit/Ion.cpp
+++ b/js/src/jit/Ion.cpp
@@ -1283,16 +1283,24 @@ GenerateCode(MIRGenerator *mir, LIRGraph
         return NULL;
 
     if (mir->compilingAsmJS()) {
         if (!codegen->generateAsmJS()) {
             js_delete(codegen);
             return NULL;
         }
     } else {
+        if (js_IonOptions.moveUnlikelyBlocks) {
+            if (!MoveUnlikelyBlocks(lir)) {
+                js_delete(codegen);
+                return NULL;
+            }
+            IonSpewPass("Move Unlikely Blocks");
+        }
+
         if (!codegen->generate()) {
             js_delete(codegen);
             return NULL;
         }
     }
 
     return codegen;
 }
diff --git a/js/src/jit/Ion.h b/js/src/jit/Ion.h
--- a/js/src/jit/Ion.h
+++ b/js/src/jit/Ion.h
@@ -188,21 +188,31 @@ struct IonOptions
     // Default: 1
     uint32_t usesBeforeCompilePar;
 
     // Whether baseline scripts are instrumented.
     //
     // Default: false
     bool baselineBranchProfiling;
 
+    // Toggles whether LIR Blocks are reordered based on branch profiles.
+    //
+    // Default: true iff baselineBranchProfiling is true.
+    bool moveUnlikelyBlocks;
+
     void setEagerCompilation() {
         eagerCompilation = true;
         usesBeforeCompile = 0;
         baselineUsesBeforeCompile = 0;
 
+        // Disable branch profiling and related optimizitions.
+        // This is for testing only.
+        baselineBranchProfiling = false;
+        moveUnlikelyBlocks = false;
+
         parallelCompilation = false;
     }
 
     IonOptions()
       : gvn(true),
         gvnIsOptimistic(true),
         licm(true),
         osr(true),
@@ -225,17 +235,20 @@ struct IonOptions
         maxInlineDepth(3),
         smallFunctionMaxInlineDepth(10),
         smallFunctionMaxBytecodeLength(100),
         polyInlineMax(4),
         inlineMaxTotalBytecodeLength(1000),
         inlineUseCountRatio(128),
         eagerCompilation(false),
         usesBeforeCompilePar(1),
-        baselineBranchProfiling(false)
+        // these modifications are for test only.
+        // it will not be included in the final patch.
+        baselineBranchProfiling(true),
+        moveUnlikelyBlocks(true)
     {
     }
 
     uint32_t usesBeforeInlining() {
         return usesBeforeCompile * usesBeforeInliningFactor;
     }
 };
 
diff --git a/js/src/jit/IonAnalysis.cpp b/js/src/jit/IonAnalysis.cpp
--- a/js/src/jit/IonAnalysis.cpp
+++ b/js/src/jit/IonAnalysis.cpp
@@ -115,16 +115,71 @@ ion::AttachBranchProfiles(MIRGraph &grap
             IonSpew(IonSpew_BranchProfiles,
                     "AttachBranchProfiles mismatch jsscript(%p) -> block %d",
                     jsscript, block->id());
         }
     }
     return true;
 }
 
+bool
+ion::MoveUnlikelyBlocks(LIRGraph *lir)
+{
+    LBlockVector likelyBlocks;
+    LBlockVector unlikelyBlocks;
+
+    lir->mir().unmarkBlocks();
+
+    // The entry block must remain in the first place.
+    JS_ASSERT(lir->numBlocks());
+    if (!likelyBlocks.append(lir->getBlock(0)))
+        return false;
+
+    // CodeGenerator::visitReturn(LReturn *lir) doesn't generate a jump
+    // when the LBlock which includes the LReturn instruction is the last LBlock
+    // in LIRGraph's block list. Because we don't want to touch CodeGenerator,
+    // the last LBlock must remain in the last place, as well as the entry block.
+    for (size_t i = 1; i < lir->numBlocks() /*- 1*/; i++) {
+        LBlock *lblock = lir->getBlock(i);
+        JS_ASSERT(lblock);
+        MBasicBlock *mblock = lblock->mir();
+        JS_ASSERT(mblock);
+
+        if (mblock->isMarked())
+            continue;
+        mblock->mark();
+
+        // TODO: Extract this condition and place it in a separated function
+        // which would be call something like "unlikelyBlockHeurisitic".
+        //
+        // Currently use this naive heuristic instead.
+        if (!mblock->isBlockUseCountAvailable() || mblock->getBlockUseCount() == 0) {
+            if (!unlikelyBlocks.append(lblock))
+                return false;
+            for (size_t j = 0; j < lblock->mir()->numSuccessors(); j++) {
+                MBasicBlock *succ = lblock->mir()->getSuccessor(j);
+                if (!succ->isMarked() && succ->lir() != lir->getBlock(lir->numBlocks() - 1) && succ->numPredecessors() == 1) {
+                    if (!unlikelyBlocks.append(succ->lir()))
+                        return false;
+                    succ->mark();
+                }
+            }
+        } else {
+            if (!likelyBlocks.append(lblock))
+                return false;
+        }
+    }
+//    if (lir->numBlocks() > 1 && !unlikelyBlocks.append(lir->getBlock(lir->numBlocks() - 1)))
+//        return false;
+
+    lir->replaceBlocksByLikelyhood(likelyBlocks, unlikelyBlocks);
+
+    return true;
+}
+
 // Operands to a resume point which are dead at the point of the resume can be
 // replaced with undefined values. This analysis supports limited detection of
 // dead operands, pruning those which are defined in the resume point's basic
 // block and have no uses outside the block or at points later than the resume
 // point.
 //
 // This is intended to ensure that extra resume points within a basic block
 // will not artificially extend the lifetimes of any SSA values. This could
diff --git a/js/src/jit/IonAnalysis.h b/js/src/jit/IonAnalysis.h
--- a/js/src/jit/IonAnalysis.h
+++ b/js/src/jit/IonAnalysis.h
@@ -20,16 +20,19 @@ class MIRGraph;
 
 bool
 SplitCriticalEdges(MIRGraph &graph);
 
 bool
 AttachBranchProfiles(MIRGraph &graph);
 
 bool
+MoveUnlikelyBlocks(LIRGraph *lir);
+
+bool
 isAsmJSCompilation(MIRGraph &graph);
 
 enum Observability {
     ConservativeObservability,
     AggressiveObservability
 };
 
 bool
diff --git a/js/src/jit/LIR.cpp b/js/src/jit/LIR.cpp
--- a/js/src/jit/LIR.cpp
+++ b/js/src/jit/LIR.cpp
@@ -45,16 +45,50 @@ LIRGraph::noteNeedsSafepoint(LInstructio
 }
 
 void
 LIRGraph::removeBlock(size_t i)
 {
     blocks_.erase(blocks_.begin() + i);
 }
 
+void
+LIRGraph::replaceBlocksByLikelyhood(LBlockVector &likelyBlocks, LBlockVector &unlikelyBlocks)
+{
+    mozilla::DebugOnly<size_t> num = numBlocks();
+    JS_ASSERT(num == likelyBlocks.length() + unlikelyBlocks.length());
+    JS_ASSERT(getBlock(0) == likelyBlocks[0]);
+
+    blocks_.clear();
+
+    mozilla::DebugOnly<bool> success;
+    success = blocks_.appendAll(likelyBlocks);
+    JS_ASSERT(success);
+    success = blocks_.appendAll(unlikelyBlocks);
+    JS_ASSERT(success);
+
+    JS_ASSERT(num == numBlocks());
+
+    renumberMBlocks();
+}
+
+bool
+LIRGraph::renumberMBlocks()
+{
+    IonSpew(IonSpew_BranchProfiles, "Entering LIRGraph::renumberMBlocks()");
+    for (size_t i = 0; i < numBlocks(); i++) {
+        MBasicBlock *block = getBlock(i)->mir();
+        JS_ASSERT(block);
+        IonSpew(IonSpew_BranchProfiles, "LBlock.MBlock.id: %d -> %d", block->id(), i);
+        block->setId(i);
+    }
+
+    return true;
+}
+
 Label *
 LBlock::label()
 {
     return begin()->toLabel()->label();
 }
 
 uint32_t
 LBlock::firstId()
diff --git a/js/src/jit/LIR.h b/js/src/jit/LIR.h
--- a/js/src/jit/LIR.h
+++ b/js/src/jit/LIR.h
@@ -723,16 +723,17 @@ class LInstructionVisitor
   public:
 #define VISIT_INS(op) virtual bool visit##op(L##op *) { MOZ_ASSUME_UNREACHABLE("NYI: " #op); }
     LIR_OPCODE_LIST(VISIT_INS)
 #undef VISIT_INS
 };
 
 typedef InlineList<LInstruction>::iterator LInstructionIterator;
 typedef InlineList<LInstruction>::reverse_iterator LInstructionReverseIterator;
+typedef Vector<LBlock *, 4, SystemAllocPolicy> LBlockVector;
 
 class LPhi;
 class LMoveGroup;
 class LBlock : public TempObject
 {
     MBasicBlock *block_;
     Vector<LPhi *, 4, IonAllocPolicy> phis_;
     InlineList<LInstruction> instructions_;
@@ -1424,16 +1425,20 @@ class LIRGraph
     }
     size_t numSafepoints() const {
         return safepoints_.length();
     }
     LInstruction *getSafepoint(size_t i) const {
         return safepoints_[i];
     }
     void removeBlock(size_t i);
+
+    void replaceBlocksByLikelyhood(LBlockVector &likelyBlocks, LBlockVector &unlikelyBlocks);
+
+    bool renumberMBlocks();
 };
 
 LAllocation::LAllocation(const AnyRegister &reg)
 {
     if (reg.isFloat())
         *this = LFloatReg(reg.fpu());
     else
         *this = LGeneralReg(reg.gpr());
diff --git a/js/src/jit/Lowering.cpp b/js/src/jit/Lowering.cpp
--- a/js/src/jit/Lowering.cpp
+++ b/js/src/jit/Lowering.cpp
@@ -309,16 +309,19 @@ LIRGenerator::visitPrepareCall(MPrepareC
 }
 
 bool
 LIRGenerator::visitPassArg(MPassArg *arg)
 {
     MDefinition *opd = arg->getArgument();
     uint32_t argslot = getArgumentSlot(arg->getArgnum());
 
+    IonSpew(IonSpew_BranchProfiles,
+            "LIRGenerator::visitPassArg(MPassArg *arg), argslot=%zu",
+            argslot);
     // Pass through the virtual register of the operand.
     // This causes snapshots to correctly copy the operand on the stack.
     //
     // This keeps the backing store around longer than strictly required.
     // We could do better by informing snapshots about the argument vector.
     arg->setVirtualRegister(opd->virtualRegister());
 
     // Values take a slow path.
diff --git a/js/src/jit/shared/CodeGenerator-shared.cpp b/js/src/jit/shared/CodeGenerator-shared.cpp
--- a/js/src/jit/shared/CodeGenerator-shared.cpp
+++ b/js/src/jit/shared/CodeGenerator-shared.cpp
@@ -545,16 +545,20 @@ void
 CodeGeneratorShared::emitPreBarrier(Address address, MIRType type)
 {
     masm.patchableCallPreBarrier(address, type);
 }
 
 void
 CodeGeneratorShared::dropArguments(unsigned argc)
 {
+    IonSpew(IonSpew_BranchProfiles,
+            "CodeGeneratorShared::dropArguments(%zu) pushedArgumentSlots_.length() = %zu",
+            argc,
+            pushedArgumentSlots_.length());
     for (unsigned i = 0; i < argc; i++)
         pushedArgumentSlots_.popBack();
 }
 
 bool
 CodeGeneratorShared::markArgumentSlots(LSafepoint *safepoint)
 {
     for (size_t i = 0; i < pushedArgumentSlots_.length(); i++) {
